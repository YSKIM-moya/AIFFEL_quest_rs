{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai-whisper pyannote-audio openai torch torchaudio librosa\n",
    "# !pip install python-dotenv\n",
    "# pip install ffmpeg\n",
    "# apt install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81e0c6",
   "metadata": {},
   "source": [
    "\n",
    "Name: torch\n",
    "Version: 2.7.1\n",
    "\n",
    "- pip install --upgrade torchvision\n",
    "Name: torchvision\n",
    "Version: 0.22.1\n",
    "\n",
    "- pip install protobuf==3.20.*\n",
    "Name: protobuf\n",
    "Version: 3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f9286",
   "metadata": {},
   "source": [
    "Hugging Face 계정 필요: https://huggingface.co\n",
    "\n",
    "Hugging Face token 발급 후 사용\n",
    "\n",
    "OpenAI GPT API 키 필요: https://platform.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9ecb8",
   "metadata": {},
   "source": [
    "👉 https://huggingface.co/pyannote/speaker-diarization 에 방문\n",
    "👉 \"Access Repository\" 버튼 클릭해서 조건에 동의\n",
    "pyannote/segmentation도 Access Repository 동의해야 함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9c00f",
   "metadata": {},
   "source": [
    "export HUGGINGFACE_TOKEN=\"hf_...\"        # Hugging Face 토큰\n",
    "\n",
    "export OPENAI_API_KEY=\"sk-...\"           # OpenAI API 키"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c942fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moya92h...\n",
      "hf_iSkSB...\n",
      "sk-proj-fUgG...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경변수 읽기\n",
    "HUGGINGFACE_USER = os.getenv(\"HUGGINGFACE_USER\")\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(HUGGINGFACE_USER[:8] + \"...\")\n",
    "print(HUGGINGFACE_TOKEN[:8] + \"...\")\n",
    "print(OPENAI_API_KEY[:12] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a6ca80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/__init__.py\n",
      "0.22.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__file__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fccff5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 Loading Whisper...\n",
      "Detected language: Korean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1041/1041 [00:00<00:00, 1781.02frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running speaker diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../aiffel/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.1+cu126. Bad things might happen unless you revert torch to 1.x.\n",
      "<class 'pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization'>\n",
      "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization object at 0x749ad511cb50>\n",
      "\n",
      "🧾 화자 구분 텍스트:\n",
      "\n",
      "SPEAKER_00: 그런 것들은 뭐 유료행 것도 있고 아닌 것도 있으니까 그런 거 좀 잘 활용하시면 좋겠고요.\n",
      "SPEAKER_00: 그리고 마지막으로 한 가지만 다요.\n",
      "SPEAKER_00: 개시판에 한번 들러보시는 것도 좋을 것 같아요.\n",
      "SPEAKER_00: 개시판에 가보면\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import openai\n",
    "from pyannote.audio import Pipeline\n",
    "import contextlib\n",
    "import wave\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "AUDIO_FILE = \"meeting.wav\"\n",
    "\n",
    "# 1. Whisper: 음성 → 텍스트\n",
    "print(\"🌀 Loading Whisper...\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "transcription = whisper_model.transcribe(AUDIO_FILE, verbose=False)\n",
    "segments = transcription['segments']\n",
    "\n",
    "\n",
    "# 2. pyannote: 화자 분리\n",
    "print(\"🔍 Running speaker diarization...\")\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "print(type(pipeline))\n",
    "print(pipeline)\n",
    "\n",
    "diarization = pipeline(AUDIO_FILE)\n",
    "\n",
    "\n",
    "# 3. 화자별 텍스트 정렬\n",
    "speaker_texts = []\n",
    "\n",
    "for segment in segments:\n",
    "    start = segment['start']\n",
    "    end = segment['end']\n",
    "    text = segment['text'].strip()\n",
    "\n",
    "    matched_speaker = None\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        if turn.start <= start <= turn.end or turn.start <= end <= turn.end:\n",
    "            matched_speaker = speaker\n",
    "            break\n",
    "    speaker_texts.append((matched_speaker or \"Unknown\", text))\n",
    "\n",
    "    \n",
    "# 4. 전체 화자별 텍스트 합치기\n",
    "combined_text = \"\"\n",
    "for speaker, text in speaker_texts:\n",
    "    combined_text += f\"{speaker}: {text}\\n\"\n",
    "\n",
    "print(\"\\n🧾 화자 구분 텍스트:\\n\")\n",
    "print(combined_text[:1000], \"...\")  # 길어서 앞부분만 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bfab0",
   "metadata": {},
   "source": [
    "from pyannote.audio import Model\n",
    "\n",
    "model = Model.from_pretrained(\n",
    "    \"pyannote/segmentation\",\n",
    "    use_auth_token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fdb19a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Running speaker diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../aiffel/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.1+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization'>\n",
      "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization object at 0x71c172ba0670>\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65af181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧾 화자 구분 텍스트:\n",
      "\n",
      "SPEAKER_00: 그런 것들은 뭐 유료행 것도 있고 아닌 것도 있으니까 그런 거 좀 잘 활용하시면 좋겠고요.\n",
      "SPEAKER_00: 그리고 마지막으로 한 가지만 다요.\n",
      "SPEAKER_00: 개시판에 한번 들러보시는 것도 좋을 것 같아요.\n",
      "SPEAKER_00: 개시판에 가보면\n",
      " ...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23172e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e988c4c4def4a2fa6de2ff4f26bb094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300f55e5883e405cb35bf89c5dc11726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c62858f3074cfca06d1cffb2ea5ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/337 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efad73cd8824ca6aa322ebfe8296172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c1c837d6fb4a1aa178c104606e20d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Running summarization using Hugging Face model...\n",
      "\n",
      " SPEAKER_00: 그런 것들은 뭐 유료행 것도 있고 아닌 것도 있으니까 그런 거 좀 잘 활용하시면 좋겠고요.\n",
      "SPEAKER_00: 그리고 마지막으로 한 가지만 다요.\n",
      "SPEAKER_00: 그리고 마지막으로 한 가지만 다요.\n",
      "SPEAKER_00: 개시판에 한번 들러보시는 것도 좋을 것 같아요.\n",
      "SPEAKER_00: 개시판에 한번 들러보시는 것도 좋을 것 같아요.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " SPEAKER_00: 그런 것들은 뭐 유료행 것도 있고 아닌 것도 있으니까 그런 거 좀 잘 활용하시면 좋겠고요.\n",
      "SPEAKER_00: 그런 것들은 뭐 유료\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 사전학습된 한국어/영어 요약모델을 쓸 수 있음\n",
    "# (영어 모델이 대표적이며, 한국어는 mBART, KoBART 등도 있음)\n",
    "#summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summarizer = pipeline(\"summarization\", model=\"hyunwoongko/kobart\")\n",
    "\n",
    "\n",
    "def summarize_with_hf(transcript):\n",
    "    print(\"🤖 Running summarization using Hugging Face model...\")\n",
    "    # 모델 입력 길이 제한 주의 (1024 토큰 이하 권장)\n",
    "    max_chunk = 1000\n",
    "    # 긴 텍스트면 여러 조각으로 나눠서 요약 후 합치기 (선택사항)\n",
    "    inputs = [transcript[i:i+max_chunk] for i in range(0, len(transcript), max_chunk)]\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in inputs:\n",
    "        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "summary = summarize_with_hf(combined_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624a92a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b4c9a84060459c92343833cb81768a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb571b9eda79454098dda6fe1e40cf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b0fc5b29ec46489b9c392baba5b2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/172k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f18cdaf96394f02a21915438a62b04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a592e166e4ec4420aaf0153bd4759cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe7b011fada405ba6187eb0a8378b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57472a4a89874610b672a49051b2cca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize: SPEAKER_00: 그런 것들은 뭐 유료행 것도 있고 아닌 것도 있으니까 그런 거 좀 잘 활용하시면 좋겠여행여행여행여행여행여행여행여행성성SPPPEAKER_00: 그리고 마지막으로 한 가지만 다마리SPPPAKER_00: 개시판에 한번 들러보시는 것도 좋을 것이라며   SPEAKER_00: 개시판에 가보면 브SPEAKER_00: 개시판에 가보면 브SPEAKER_00: 개시판에 가보면 브SPEAKER_00: 개시판에 가보면 브SPEAKER_00: 개시판에 가보면 브SPEAKER_00: 개시판에 가보면 가보면 브SPEAKER_00: 그런 것들은 뭐 유료행 것도 있고 아닌 것도 있고 아닌 것도 있으니까 그런 것들은 유료행 것도 있고 아닌 것도 있으니까 그런 거를 좀 잘 활용하시면 좋겠여행여행여행여행여행여행여행여행여행여행여행여행여행여행여행하고 마지막으로 한 가지만 다 같이 한 가지만 다마리SPPPPPPPPEKER_00: 개시판에 한번 들러보\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 👉 Hugging Face 모델로 요약\n",
    "def summarize_with_hf(text):\n",
    "    model_name = \"gogamza/kobart-summarization\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HUGGINGFACE_TOKEN)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=256, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 사용 예시\n",
    "summary = summarize_with_hf(combined_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18aedff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "회의 요약: Summarize: summarize:  summarize:  summarize:   summarize:   summarize:   summarize:   summarize:  summarize:   summarize: 마케팅 예산을 늘리는 것에 동의하며 마케팅 예산을 늘리는 것에 동의하며 김철수: 마케팅 예산을 늘리는 것에 동의하며 마케팅 예산을 늘리는 것에 동의하며 마케팅 예산을 늘리는 것에 동의하는             ummarize:    summarize:    summarize:   summarize:                  예산 조정 안건은 예산 조정예산 조정팀은 예산 조정팀은 예산 조정자문- 이영희: 예산을 10% 삭감하는 것이 필요해지고  박민수: 삭감된 예산은 삭감된 예산은 삭감된 예산은 삭감된 예산은 마케팅에 재배분해야  김철수: 마케팅 예산을 늘리는 것에 동의\n"
     ]
    }
   ],
   "source": [
    "transcript = \"\"\"\n",
    "회의록:\n",
    "- 김철수: 오늘 회의의 주요 안건은 예산 조정입니다.\n",
    "- 이영희: 예산을 10% 삭감하는 것이 필요합니다.\n",
    "- 박민수: 삭감된 예산은 마케팅에 재배분해야 합니다.\n",
    "- 김철수: 마케팅 예산을 늘리는 것에 동의합니다.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_with_hf(transcript)\n",
    "print(\"회의 요약:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2990d482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7428bec5ab24e148d79def8634f1dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066503c2cbbb497c86ef515236a84bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253bc066e0e04125a2e2ee903a53c4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5706cf867b2e4f00b008357e349e8344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0f6c2cb8d34205976cbe0c4c5da1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Running summarization using Hugging Face model...\n",
      "SPEAKER_00:    ‘ 활용하’   ‘  ’’ ‘”  ’‘ ’ ’  ” ‘ ‘.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# KoBART 요약 모델 로드\n",
    "model_name = \"digit82/kobart-summarization\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def summarize_with_kobart(text):\n",
    "    print(\"📝 Hugging Face KoBART로 요약 중...\")\n",
    "\n",
    "    # 입력 토큰화\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # 모델로 요약 생성\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        min_length=32,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # 디코딩\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "summary = summarize_with_hf(combined_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea3f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📩 Sending to GPT for summarization...\n",
      "\n",
      "✅ 요약 결과:\n",
      "\n",
      "주요 논의사항:\n",
      "- 유료 및 무료로 활용할 수 있는 다양한 자료/도구가 있으니 적절히 활용할 것을 권장함(SPEAKER_00).\n",
      "- 개시판(게시판) 방문을 추천하며, 게시판 활용의 필요성 언급(SPEAKER_00).\n",
      "\n",
      "의사결정:\n",
      "- 별도의 명확한 의사결정 내용은 없음.\n",
      "\n",
      "액션 아이템:\n",
      "- 참여자들이 유료/무료 자료 및 도구를 적극적으로 활용할 것.\n",
      "- 게시판에 방문하여 정보를 확인할 것.\n"
     ]
    }
   ],
   "source": [
    "#### 5. GPT 요약 요청\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def summarize_with_gpt(transcript):\n",
    "    print(\"📩 Sending to GPT for summarization...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",  # 또는 gpt-3.5-turbo\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 회의록 정리 도우미입니다. 대화 내용을 요약해 주세요. 화자 구분이 포함되어 있습니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"다음은 화자별로 정리된 회의 내용입니다. 주요 논의사항, 의사결정, 액션 아이템을 요약해 주세요:\\n\\n{transcript}\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "   \n",
    "\n",
    "summary = summarize_with_gpt(combined_text)\n",
    "\n",
    "# 6. 결과 출력\n",
    "print(\"\\n✅ 요약 결과:\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1935f",
   "metadata": {},
   "source": [
    "\n",
    "🧾 화자 구분 텍스트:\n",
    "\n",
    "SPEAKER_00: 그런 것들은 뭐 유료행 것도 있고 아닌 것도 있으니까 그런 거 좀 잘 활용하시면 좋겠고요.\n",
    "SPEAKER_00: 그리고 마지막으로 한 가지만 다요.\n",
    "SPEAKER_00: 개시판에 한번 들러보시는 것도 좋을 것 같아요.\n",
    "SPEAKER_00: 개시판에 가보면\n",
    "\n",
    "\n",
    "\n",
    "✅ 요약 결과:\n",
    "\n",
    "주요 논의사항:\n",
    "- 유료 및 무료로 활용할 수 있는 다양한 자료/도구가 있으니 적절히 활용할 것을 권장함(SPEAKER_00).\n",
    "- 개시판(게시판) 방문을 추천하며, 게시판 활용의 필요성 언급(SPEAKER_00).\n",
    "\n",
    "의사결정:\n",
    "- 별도의 명확한 의사결정 내용은 없음.\n",
    "\n",
    "액션 아이템:\n",
    "- 참여자들이 유료/무료 자료 및 도구를 적극적으로 활용할 것.\n",
    "- 게시판에 방문하여 정보를 확인할 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "497dd5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Hugging Face KoBART로 요약 중...\n",
      "📌 요약 결과:\n",
      " 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠고 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠고 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠고 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠고 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠고 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠고 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠고 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠다며 신규회의 시작하겠다.\n"
     ]
    }
   ],
   "source": [
    "transcript = \"\"\"\n",
    "회의 시작하겠습니다. 오늘은 신규 서비스 론칭 일정과 마케팅 전략을 논의하겠습니다.\n",
    "김 팀장님, 현재 개발 진행 상황 공유해 주세요.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_with_kobart(transcript)\n",
    "print(\"📌 요약 결과:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40aa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit_app.py\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"🎙️ 자동 회의 요약기\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"회의 음성 파일 업로드\", type=[\"wav\", \"mp3\"])\n",
    "if uploaded_file:\n",
    "    audio_path = \"uploaded.wav\"\n",
    "    with open(audio_path, \"wb\") as f:\n",
    "        f.write(uploaded_file.read())\n",
    "\n",
    "    text = transcribe_audio(audio_path)\n",
    "    segments = diarize_speakers(audio_path, os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "    combined = assemble_transcript(text, segments)\n",
    "    summary = summarize_with_hf(combined)\n",
    "\n",
    "    st.subheader(\"📝 요약 결과\")\n",
    "    st.write(summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52295e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌀 Loading Whisper...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ffmpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_229/133308620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"🌀 Loading Whisper...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwhisper_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtranscription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUDIO_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Pad 30-seconds of silence to the input audio, for slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mcontent_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mN_FRAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mcontent_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_frames\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHOP_LENGTH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/whisper/audio.py\u001b[0m in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio: {e.stderr.decode()}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1819\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import openai\n",
    "from pyannote.audio import Pipeline\n",
    "import contextlib\n",
    "import wave\n",
    "\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "AUDIO_FILE = \"meeting.wav\"\n",
    "\n",
    "# 1. Whisper: 음성 → 텍스트\n",
    "print(\"🌀 Loading Whisper...\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "transcription = whisper_model.transcribe(AUDIO_FILE, verbose=False)\n",
    "segments = transcription['segments']\n",
    "\n",
    "# 2. pyannote: 화자 분리\n",
    "print(\"🔍 Running speaker diarization...\")\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=HUGGINGFACE_TOKEN)\n",
    "diarization = pipeline(AUDIO_FILE)\n",
    "\n",
    "# 3. 화자별 텍스트 정렬\n",
    "speaker_texts = []\n",
    "\n",
    "for segment in segments:\n",
    "    start = segment['start']\n",
    "    end = segment['end']\n",
    "    text = segment['text'].strip()\n",
    "\n",
    "    matched_speaker = None\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        if turn.start <= start <= turn.end or turn.start <= end <= turn.end:\n",
    "            matched_speaker = speaker\n",
    "            break\n",
    "    speaker_texts.append((matched_speaker or \"Unknown\", text))\n",
    "\n",
    "# 4. 전체 화자별 텍스트 합치기\n",
    "combined_text = \"\"\n",
    "for speaker, text in speaker_texts:\n",
    "    combined_text += f\"{speaker}: {text}\\n\"\n",
    "\n",
    "print(\"\\n🧾 화자 구분 텍스트:\\n\")\n",
    "print(combined_text[:1000], \"...\")  # 길어서 앞부분만 출력\n",
    "\n",
    "# 5. GPT 요약 요청\n",
    "def summarize_with_gpt(transcript):\n",
    "    print(\"📩 Sending to GPT for summarization...\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",  # 또는 gpt-3.5-turbo\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"당신은 회의록 정리 도우미입니다. 대화 내용을 요약해 주세요. 화자 구분이 포함되어 있습니다.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"다음은 화자별로 정리된 회의 내용입니다. 주요 논의사항, 의사결정, 액션 아이템을 요약해 주세요:\\n\\n{transcript}\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "summary = summarize_with_gpt(combined_text)\n",
    "\n",
    "# 6. 결과 출력\n",
    "print(\"\\n✅ 요약 결과:\\n\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

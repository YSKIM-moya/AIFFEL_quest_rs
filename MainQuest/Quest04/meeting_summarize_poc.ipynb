{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai-whisper pyannote-audio openai torch torchaudio librosa\n",
    "# !pip install python-dotenv\n",
    "# pip install ffmpeg\n",
    "# apt install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81e0c6",
   "metadata": {},
   "source": [
    "\n",
    "Name: torch\n",
    "Version: 2.7.1\n",
    "\n",
    "- pip install --upgrade torchvision\n",
    "Name: torchvision\n",
    "Version: 0.22.1\n",
    "\n",
    "- pip install protobuf==3.20.*\n",
    "Name: protobuf\n",
    "Version: 3.20.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f9286",
   "metadata": {},
   "source": [
    "Hugging Face ê³„ì • í•„ìš”: https://huggingface.co\n",
    "\n",
    "Hugging Face token ë°œê¸‰ í›„ ì‚¬ìš©\n",
    "\n",
    "OpenAI GPT API í‚¤ í•„ìš”: https://platform.openai.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9ecb8",
   "metadata": {},
   "source": [
    "ğŸ‘‰ https://huggingface.co/pyannote/speaker-diarization ì— ë°©ë¬¸\n",
    "ğŸ‘‰ \"Access Repository\" ë²„íŠ¼ í´ë¦­í•´ì„œ ì¡°ê±´ì— ë™ì˜\n",
    "pyannote/segmentationë„ Access Repository ë™ì˜í•´ì•¼ í•¨.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9c00f",
   "metadata": {},
   "source": [
    "export HUGGINGFACE_TOKEN=\"hf_...\"        # Hugging Face í† í°\n",
    "\n",
    "export OPENAI_API_KEY=\"sk-...\"           # OpenAI API í‚¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c942fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moya92h...\n",
      "hf_iSkSB...\n",
      "sk-proj-fUgG...\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env íŒŒì¼ ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ì½ê¸°\n",
    "HUGGINGFACE_USER = os.getenv(\"HUGGINGFACE_USER\")\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(HUGGINGFACE_USER[:8] + \"...\")\n",
    "print(HUGGINGFACE_TOKEN[:8] + \"...\")\n",
    "print(OPENAI_API_KEY[:12] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a6ca80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/__init__.py\n",
      "0.22.1+cu126\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__file__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fccff5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ€ Loading Whisper...\n",
      "Detected language: Korean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1041/1041 [00:00<00:00, 1781.02frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Running speaker diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../aiffel/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.1+cu126. Bad things might happen unless you revert torch to 1.x.\n",
      "<class 'pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization'>\n",
      "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization object at 0x749ad511cb50>\n",
      "\n",
      "ğŸ§¾ í™”ì êµ¬ë¶„ í…ìŠ¤íŠ¸:\n",
      "\n",
      "SPEAKER_00: ê·¸ëŸ° ê²ƒë“¤ì€ ë­ ìœ ë£Œí–‰ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆìœ¼ë‹ˆê¹Œ ê·¸ëŸ° ê±° ì¢€ ì˜ í™œìš©í•˜ì‹œë©´ ì¢‹ê² ê³ ìš”.\n",
      "SPEAKER_00: ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€ë§Œ ë‹¤ìš”.\n",
      "SPEAKER_00: ê°œì‹œíŒì— í•œë²ˆ ë“¤ëŸ¬ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.\n",
      "SPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import openai\n",
    "from pyannote.audio import Pipeline\n",
    "import contextlib\n",
    "import wave\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "AUDIO_FILE = \"meeting.wav\"\n",
    "\n",
    "# 1. Whisper: ìŒì„± â†’ í…ìŠ¤íŠ¸\n",
    "print(\"ğŸŒ€ Loading Whisper...\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "transcription = whisper_model.transcribe(AUDIO_FILE, verbose=False)\n",
    "segments = transcription['segments']\n",
    "\n",
    "\n",
    "# 2. pyannote: í™”ì ë¶„ë¦¬\n",
    "print(\"ğŸ” Running speaker diarization...\")\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "print(type(pipeline))\n",
    "print(pipeline)\n",
    "\n",
    "diarization = pipeline(AUDIO_FILE)\n",
    "\n",
    "\n",
    "# 3. í™”ìë³„ í…ìŠ¤íŠ¸ ì •ë ¬\n",
    "speaker_texts = []\n",
    "\n",
    "for segment in segments:\n",
    "    start = segment['start']\n",
    "    end = segment['end']\n",
    "    text = segment['text'].strip()\n",
    "\n",
    "    matched_speaker = None\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        if turn.start <= start <= turn.end or turn.start <= end <= turn.end:\n",
    "            matched_speaker = speaker\n",
    "            break\n",
    "    speaker_texts.append((matched_speaker or \"Unknown\", text))\n",
    "\n",
    "    \n",
    "# 4. ì „ì²´ í™”ìë³„ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "combined_text = \"\"\n",
    "for speaker, text in speaker_texts:\n",
    "    combined_text += f\"{speaker}: {text}\\n\"\n",
    "\n",
    "print(\"\\nğŸ§¾ í™”ì êµ¬ë¶„ í…ìŠ¤íŠ¸:\\n\")\n",
    "print(combined_text[:1000], \"...\")  # ê¸¸ì–´ì„œ ì•ë¶€ë¶„ë§Œ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bfab0",
   "metadata": {},
   "source": [
    "from pyannote.audio import Model\n",
    "\n",
    "model = Model.from_pretrained(\n",
    "    \"pyannote/segmentation\",\n",
    "    use_auth_token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fdb19a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Running speaker diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../../../aiffel/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.1+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization'>\n",
      "<pyannote.audio.pipelines.speaker_diarization.SpeakerDiarization object at 0x71c172ba0670>\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c65af181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§¾ í™”ì êµ¬ë¶„ í…ìŠ¤íŠ¸:\n",
      "\n",
      "SPEAKER_00: ê·¸ëŸ° ê²ƒë“¤ì€ ë­ ìœ ë£Œí–‰ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆìœ¼ë‹ˆê¹Œ ê·¸ëŸ° ê±° ì¢€ ì˜ í™œìš©í•˜ì‹œë©´ ì¢‹ê² ê³ ìš”.\n",
      "SPEAKER_00: ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€ë§Œ ë‹¤ìš”.\n",
      "SPEAKER_00: ê°œì‹œíŒì— í•œë²ˆ ë“¤ëŸ¬ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.\n",
      "SPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´\n",
      " ...\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23172e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e988c4c4def4a2fa6de2ff4f26bb094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300f55e5883e405cb35bf89c5dc11726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98c62858f3074cfca06d1cffb2ea5ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/337 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efad73cd8824ca6aa322ebfe8296172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c1c837d6fb4a1aa178c104606e20d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Running summarization using Hugging Face model...\n",
      "\n",
      " SPEAKER_00: ê·¸ëŸ° ê²ƒë“¤ì€ ë­ ìœ ë£Œí–‰ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆìœ¼ë‹ˆê¹Œ ê·¸ëŸ° ê±° ì¢€ ì˜ í™œìš©í•˜ì‹œë©´ ì¢‹ê² ê³ ìš”.\n",
      "SPEAKER_00: ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€ë§Œ ë‹¤ìš”.\n",
      "SPEAKER_00: ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€ë§Œ ë‹¤ìš”.\n",
      "SPEAKER_00: ê°œì‹œíŒì— í•œë²ˆ ë“¤ëŸ¬ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.\n",
      "SPEAKER_00: ê°œì‹œíŒì— í•œë²ˆ ë“¤ëŸ¬ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " SPEAKER_00: ê·¸ëŸ° ê²ƒë“¤ì€ ë­ ìœ ë£Œí–‰ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆìœ¼ë‹ˆê¹Œ ê·¸ëŸ° ê±° ì¢€ ì˜ í™œìš©í•˜ì‹œë©´ ì¢‹ê² ê³ ìš”.\n",
      "SPEAKER_00: ê·¸ëŸ° ê²ƒë“¤ì€ ë­ ìœ ë£Œ\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ì‚¬ì „í•™ìŠµëœ í•œêµ­ì–´/ì˜ì–´ ìš”ì•½ëª¨ë¸ì„ ì“¸ ìˆ˜ ìˆìŒ\n",
    "# (ì˜ì–´ ëª¨ë¸ì´ ëŒ€í‘œì ì´ë©°, í•œêµ­ì–´ëŠ” mBART, KoBART ë“±ë„ ìˆìŒ)\n",
    "#summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "summarizer = pipeline(\"summarization\", model=\"hyunwoongko/kobart\")\n",
    "\n",
    "\n",
    "def summarize_with_hf(transcript):\n",
    "    print(\"ğŸ¤– Running summarization using Hugging Face model...\")\n",
    "    # ëª¨ë¸ ì…ë ¥ ê¸¸ì´ ì œí•œ ì£¼ì˜ (1024 í† í° ì´í•˜ ê¶Œì¥)\n",
    "    max_chunk = 1000\n",
    "    # ê¸´ í…ìŠ¤íŠ¸ë©´ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ ì„œ ìš”ì•½ í›„ í•©ì¹˜ê¸° (ì„ íƒì‚¬í•­)\n",
    "    inputs = [transcript[i:i+max_chunk] for i in range(0, len(transcript), max_chunk)]\n",
    "\n",
    "    summaries = []\n",
    "    for chunk in inputs:\n",
    "        summary = summarizer(chunk, max_length=150, min_length=40, do_sample=False)[0]['summary_text']\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return \" \".join(summaries)\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "summary = summarize_with_hf(combined_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624a92a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b4c9a84060459c92343833cb81768a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb571b9eda79454098dda6fe1e40cf3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b0fc5b29ec46489b9c392baba5b2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/172k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f18cdaf96394f02a21915438a62b04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a592e166e4ec4420aaf0153bd4759cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe7b011fada405ba6187eb0a8378b15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57472a4a89874610b672a49051b2cca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize: SPEAKER_00: ê·¸ëŸ° ê²ƒë“¤ì€ ë­ ìœ ë£Œí–‰ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆìœ¼ë‹ˆê¹Œ ê·¸ëŸ° ê±° ì¢€ ì˜ í™œìš©í•˜ì‹œë©´ ì¢‹ê² ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì„±ì„±SPPPEAKER_00: ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€ë§Œ ë‹¤ë§ˆë¦¬SPPPAKER_00: ê°œì‹œíŒì— í•œë²ˆ ë“¤ëŸ¬ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒì´ë¼ë©°   SPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´ ë¸ŒSPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´ ë¸ŒSPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´ ë¸ŒSPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´ ë¸ŒSPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´ ë¸ŒSPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´ ê°€ë³´ë©´ ë¸ŒSPEAKER_00: ê·¸ëŸ° ê²ƒë“¤ì€ ë­ ìœ ë£Œí–‰ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆìœ¼ë‹ˆê¹Œ ê·¸ëŸ° ê²ƒë“¤ì€ ìœ ë£Œí–‰ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆìœ¼ë‹ˆê¹Œ ê·¸ëŸ° ê±°ë¥¼ ì¢€ ì˜ í™œìš©í•˜ì‹œë©´ ì¢‹ê² ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰ì—¬í–‰í•˜ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€ë§Œ ë‹¤ ê°™ì´ í•œ ê°€ì§€ë§Œ ë‹¤ë§ˆë¦¬SPPPPPPPPEKER_00: ê°œì‹œíŒì— í•œë²ˆ ë“¤ëŸ¬ë³´\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# ğŸ‘‰ Hugging Face ëª¨ë¸ë¡œ ìš”ì•½\n",
    "def summarize_with_hf(text):\n",
    "    model_name = \"gogamza/kobart-summarization\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=HUGGINGFACE_TOKEN)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name, use_auth_token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(inputs, max_length=256, min_length=30, length_penalty=2.0, num_beams=4)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "summary = summarize_with_hf(combined_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c18aedff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "íšŒì˜ ìš”ì•½: Summarize: summarize:  summarize:  summarize:   summarize:   summarize:   summarize:   summarize:  summarize:   summarize: ë§ˆì¼€íŒ… ì˜ˆì‚°ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì— ë™ì˜í•˜ë©° ë§ˆì¼€íŒ… ì˜ˆì‚°ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì— ë™ì˜í•˜ë©° ê¹€ì² ìˆ˜: ë§ˆì¼€íŒ… ì˜ˆì‚°ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì— ë™ì˜í•˜ë©° ë§ˆì¼€íŒ… ì˜ˆì‚°ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì— ë™ì˜í•˜ë©° ë§ˆì¼€íŒ… ì˜ˆì‚°ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì— ë™ì˜í•˜ëŠ”             ummarize:    summarize:    summarize:   summarize:                  ì˜ˆì‚° ì¡°ì • ì•ˆê±´ì€ ì˜ˆì‚° ì¡°ì •ì˜ˆì‚° ì¡°ì •íŒ€ì€ ì˜ˆì‚° ì¡°ì •íŒ€ì€ ì˜ˆì‚° ì¡°ì •ìë¬¸- ì´ì˜í¬: ì˜ˆì‚°ì„ 10% ì‚­ê°í•˜ëŠ” ê²ƒì´ í•„ìš”í•´ì§€ê³   ë°•ë¯¼ìˆ˜: ì‚­ê°ëœ ì˜ˆì‚°ì€ ì‚­ê°ëœ ì˜ˆì‚°ì€ ì‚­ê°ëœ ì˜ˆì‚°ì€ ì‚­ê°ëœ ì˜ˆì‚°ì€ ë§ˆì¼€íŒ…ì— ì¬ë°°ë¶„í•´ì•¼  ê¹€ì² ìˆ˜: ë§ˆì¼€íŒ… ì˜ˆì‚°ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì— ë™ì˜\n"
     ]
    }
   ],
   "source": [
    "transcript = \"\"\"\n",
    "íšŒì˜ë¡:\n",
    "- ê¹€ì² ìˆ˜: ì˜¤ëŠ˜ íšŒì˜ì˜ ì£¼ìš” ì•ˆê±´ì€ ì˜ˆì‚° ì¡°ì •ì…ë‹ˆë‹¤.\n",
    "- ì´ì˜í¬: ì˜ˆì‚°ì„ 10% ì‚­ê°í•˜ëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "- ë°•ë¯¼ìˆ˜: ì‚­ê°ëœ ì˜ˆì‚°ì€ ë§ˆì¼€íŒ…ì— ì¬ë°°ë¶„í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "- ê¹€ì² ìˆ˜: ë§ˆì¼€íŒ… ì˜ˆì‚°ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì— ë™ì˜í•©ë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_with_hf(transcript)\n",
    "print(\"íšŒì˜ ìš”ì•½:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2990d482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7428bec5ab24e148d79def8634f1dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/109 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066503c2cbbb497c86ef515236a84bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253bc066e0e04125a2e2ee903a53c4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/666k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5706cf867b2e4f00b008357e349e8344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0f6c2cb8d34205976cbe0c4c5da1cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– Running summarization using Hugging Face model...\n",
      "SPEAKER_00:   Â â€˜ í™œìš©í•˜â€™   â€˜  â€™â€™ â€˜â€  â€™â€˜ â€™ â€™  â€ â€˜ â€˜.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
    "\n",
    "# KoBART ìš”ì•½ ëª¨ë¸ ë¡œë“œ\n",
    "model_name = \"digit82/kobart-summarization\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def summarize_with_kobart(text):\n",
    "    print(\"ğŸ“ Hugging Face KoBARTë¡œ ìš”ì•½ ì¤‘...\")\n",
    "\n",
    "    # ì…ë ¥ í† í°í™”\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "\n",
    "    # ëª¨ë¸ë¡œ ìš”ì•½ ìƒì„±\n",
    "    summary_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        min_length=32,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # ë””ì½”ë”©\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "summary = summarize_with_hf(combined_text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea3f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“© Sending to GPT for summarization...\n",
      "\n",
      "âœ… ìš”ì•½ ê²°ê³¼:\n",
      "\n",
      "ì£¼ìš” ë…¼ì˜ì‚¬í•­:\n",
      "- ìœ ë£Œ ë° ë¬´ë£Œë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìë£Œ/ë„êµ¬ê°€ ìˆìœ¼ë‹ˆ ì ì ˆíˆ í™œìš©í•  ê²ƒì„ ê¶Œì¥í•¨(SPEAKER_00).\n",
      "- ê°œì‹œíŒ(ê²Œì‹œíŒ) ë°©ë¬¸ì„ ì¶”ì²œí•˜ë©°, ê²Œì‹œíŒ í™œìš©ì˜ í•„ìš”ì„± ì–¸ê¸‰(SPEAKER_00).\n",
      "\n",
      "ì˜ì‚¬ê²°ì •:\n",
      "- ë³„ë„ì˜ ëª…í™•í•œ ì˜ì‚¬ê²°ì • ë‚´ìš©ì€ ì—†ìŒ.\n",
      "\n",
      "ì•¡ì…˜ ì•„ì´í…œ:\n",
      "- ì°¸ì—¬ìë“¤ì´ ìœ ë£Œ/ë¬´ë£Œ ìë£Œ ë° ë„êµ¬ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•  ê²ƒ.\n",
      "- ê²Œì‹œíŒì— ë°©ë¬¸í•˜ì—¬ ì •ë³´ë¥¼ í™•ì¸í•  ê²ƒ.\n"
     ]
    }
   ],
   "source": [
    "#### 5. GPT ìš”ì•½ ìš”ì²­\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def summarize_with_gpt(transcript):\n",
    "    print(\"ğŸ“© Sending to GPT for summarization...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",  # ë˜ëŠ” gpt-3.5-turbo\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"ë‹¹ì‹ ì€ íšŒì˜ë¡ ì •ë¦¬ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”. í™”ì êµ¬ë¶„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"ë‹¤ìŒì€ í™”ìë³„ë¡œ ì •ë¦¬ëœ íšŒì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì£¼ìš” ë…¼ì˜ì‚¬í•­, ì˜ì‚¬ê²°ì •, ì•¡ì…˜ ì•„ì´í…œì„ ìš”ì•½í•´ ì£¼ì„¸ìš”:\\n\\n{transcript}\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "   \n",
    "\n",
    "summary = summarize_with_gpt(combined_text)\n",
    "\n",
    "# 6. ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nâœ… ìš”ì•½ ê²°ê³¼:\\n\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1935f",
   "metadata": {},
   "source": [
    "\n",
    "ğŸ§¾ í™”ì êµ¬ë¶„ í…ìŠ¤íŠ¸:\n",
    "\n",
    "SPEAKER_00: ê·¸ëŸ° ê²ƒë“¤ì€ ë­ ìœ ë£Œí–‰ ê²ƒë„ ìˆê³  ì•„ë‹Œ ê²ƒë„ ìˆìœ¼ë‹ˆê¹Œ ê·¸ëŸ° ê±° ì¢€ ì˜ í™œìš©í•˜ì‹œë©´ ì¢‹ê² ê³ ìš”.\n",
    "SPEAKER_00: ê·¸ë¦¬ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ í•œ ê°€ì§€ë§Œ ë‹¤ìš”.\n",
    "SPEAKER_00: ê°œì‹œíŒì— í•œë²ˆ ë“¤ëŸ¬ë³´ì‹œëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.\n",
    "SPEAKER_00: ê°œì‹œíŒì— ê°€ë³´ë©´\n",
    "\n",
    "\n",
    "\n",
    "âœ… ìš”ì•½ ê²°ê³¼:\n",
    "\n",
    "ì£¼ìš” ë…¼ì˜ì‚¬í•­:\n",
    "- ìœ ë£Œ ë° ë¬´ë£Œë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ìë£Œ/ë„êµ¬ê°€ ìˆìœ¼ë‹ˆ ì ì ˆíˆ í™œìš©í•  ê²ƒì„ ê¶Œì¥í•¨(SPEAKER_00).\n",
    "- ê°œì‹œíŒ(ê²Œì‹œíŒ) ë°©ë¬¸ì„ ì¶”ì²œí•˜ë©°, ê²Œì‹œíŒ í™œìš©ì˜ í•„ìš”ì„± ì–¸ê¸‰(SPEAKER_00).\n",
    "\n",
    "ì˜ì‚¬ê²°ì •:\n",
    "- ë³„ë„ì˜ ëª…í™•í•œ ì˜ì‚¬ê²°ì • ë‚´ìš©ì€ ì—†ìŒ.\n",
    "\n",
    "ì•¡ì…˜ ì•„ì´í…œ:\n",
    "- ì°¸ì—¬ìë“¤ì´ ìœ ë£Œ/ë¬´ë£Œ ìë£Œ ë° ë„êµ¬ë¥¼ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•  ê²ƒ.\n",
    "- ê²Œì‹œíŒì— ë°©ë¬¸í•˜ì—¬ ì •ë³´ë¥¼ í™•ì¸í•  ê²ƒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "497dd5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Hugging Face KoBARTë¡œ ìš”ì•½ ì¤‘...\n",
      "ğŸ“Œ ìš”ì•½ ê²°ê³¼:\n",
      " ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ê³  ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ê³  ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ê³  ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ê³  ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ê³  ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ê³  ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ê³  ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ë‹¤ë©° ì‹ ê·œíšŒì˜ ì‹œì‘í•˜ê² ë‹¤.\n"
     ]
    }
   ],
   "source": [
    "transcript = \"\"\"\n",
    "íšŒì˜ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ì‹ ê·œ ì„œë¹„ìŠ¤ ë¡ ì¹­ ì¼ì •ê³¼ ë§ˆì¼€íŒ… ì „ëµì„ ë…¼ì˜í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "ê¹€ íŒ€ì¥ë‹˜, í˜„ì¬ ê°œë°œ ì§„í–‰ ìƒí™© ê³µìœ í•´ ì£¼ì„¸ìš”.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarize_with_kobart(transcript)\n",
    "print(\"ğŸ“Œ ìš”ì•½ ê²°ê³¼:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40aa936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamlit_app.py\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"ğŸ™ï¸ ìë™ íšŒì˜ ìš”ì•½ê¸°\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"íšŒì˜ ìŒì„± íŒŒì¼ ì—…ë¡œë“œ\", type=[\"wav\", \"mp3\"])\n",
    "if uploaded_file:\n",
    "    audio_path = \"uploaded.wav\"\n",
    "    with open(audio_path, \"wb\") as f:\n",
    "        f.write(uploaded_file.read())\n",
    "\n",
    "    text = transcribe_audio(audio_path)\n",
    "    segments = diarize_speakers(audio_path, os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "    combined = assemble_transcript(text, segments)\n",
    "    summary = summarize_with_hf(combined)\n",
    "\n",
    "    st.subheader(\"ğŸ“ ìš”ì•½ ê²°ê³¼\")\n",
    "    st.write(summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52295e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ€ Loading Whisper...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ffmpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_229/133308620.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸŒ€ Loading Whisper...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mwhisper_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"base\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtranscription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUDIO_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0msegments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscription\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segments'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/whisper/transcribe.py\u001b[0m in \u001b[0;36mtranscribe\u001b[0;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Pad 30-seconds of silence to the input audio, for slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mmel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_mel_spectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_mels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mcontent_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mN_FRAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mcontent_duration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_frames\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mHOP_LENGTH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/whisper/audio.py\u001b[0m in \u001b[0;36mlog_mel_spectrogram\u001b[0;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/whisper/audio.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(file, sr)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load audio: {e.stderr.decode()}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1819\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import openai\n",
    "from pyannote.audio import Pipeline\n",
    "import contextlib\n",
    "import wave\n",
    "\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "AUDIO_FILE = \"meeting.wav\"\n",
    "\n",
    "# 1. Whisper: ìŒì„± â†’ í…ìŠ¤íŠ¸\n",
    "print(\"ğŸŒ€ Loading Whisper...\")\n",
    "whisper_model = whisper.load_model(\"base\")\n",
    "transcription = whisper_model.transcribe(AUDIO_FILE, verbose=False)\n",
    "segments = transcription['segments']\n",
    "\n",
    "# 2. pyannote: í™”ì ë¶„ë¦¬\n",
    "print(\"ğŸ” Running speaker diarization...\")\n",
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=HUGGINGFACE_TOKEN)\n",
    "diarization = pipeline(AUDIO_FILE)\n",
    "\n",
    "# 3. í™”ìë³„ í…ìŠ¤íŠ¸ ì •ë ¬\n",
    "speaker_texts = []\n",
    "\n",
    "for segment in segments:\n",
    "    start = segment['start']\n",
    "    end = segment['end']\n",
    "    text = segment['text'].strip()\n",
    "\n",
    "    matched_speaker = None\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        if turn.start <= start <= turn.end or turn.start <= end <= turn.end:\n",
    "            matched_speaker = speaker\n",
    "            break\n",
    "    speaker_texts.append((matched_speaker or \"Unknown\", text))\n",
    "\n",
    "# 4. ì „ì²´ í™”ìë³„ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "combined_text = \"\"\n",
    "for speaker, text in speaker_texts:\n",
    "    combined_text += f\"{speaker}: {text}\\n\"\n",
    "\n",
    "print(\"\\nğŸ§¾ í™”ì êµ¬ë¶„ í…ìŠ¤íŠ¸:\\n\")\n",
    "print(combined_text[:1000], \"...\")  # ê¸¸ì–´ì„œ ì•ë¶€ë¶„ë§Œ ì¶œë ¥\n",
    "\n",
    "# 5. GPT ìš”ì•½ ìš”ì²­\n",
    "def summarize_with_gpt(transcript):\n",
    "    print(\"ğŸ“© Sending to GPT for summarization...\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",  # ë˜ëŠ” gpt-3.5-turbo\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"ë‹¹ì‹ ì€ íšŒì˜ë¡ ì •ë¦¬ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”. í™”ì êµ¬ë¶„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"ë‹¤ìŒì€ í™”ìë³„ë¡œ ì •ë¦¬ëœ íšŒì˜ ë‚´ìš©ì…ë‹ˆë‹¤. ì£¼ìš” ë…¼ì˜ì‚¬í•­, ì˜ì‚¬ê²°ì •, ì•¡ì…˜ ì•„ì´í…œì„ ìš”ì•½í•´ ì£¼ì„¸ìš”:\\n\\n{transcript}\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=800,\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "summary = summarize_with_gpt(combined_text)\n",
    "\n",
    "# 6. ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nâœ… ìš”ì•½ ê²°ê³¼:\\n\")\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f20c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0027afe3",
   "metadata": {},
   "source": [
    "## 1. Tokenizer 준비\n",
    "8000의 vocab_size를 갖는 sentencepiece 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc5c827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "data_dir = './data'\n",
    "model_dir = './models'\n",
    "\n",
    "model_prefix = model_dir + '/ko_8000'\n",
    "\n",
    "pretrain_json_path = data_dir + '/bert_pre_train.json'\n",
    "\n",
    "vocab_size = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df37351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/ko_8000\n",
      "./data/bert_pre_train.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./models/ko_8000'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_prefix)\n",
    "print(pretrain_json_path)\n",
    "mm = f\"{model_dir}/ko_8000\"\n",
    "mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dcc8a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/bert_pretrain/data/kowiki.txt                                --model_prefix=models/ko_8000                                --vocab_size=8007                                --model_type=bpe                                --max_sentence_length=999999                                --pad_id=0 --pad_piece=[PAD]                                --unk_id=1 --unk_piece=[UNK]                                --bos_id=2 --bos_piece=[BOS]                                --eos_id=3 --eos_piece=[EOS]                                --user_defined_symbols=[SEP],[CLS],[MASK]\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "  input_format: \n",
      "  model_prefix: models/ko_8000\n",
      "  model_type: BPE\n",
      "  vocab_size: 8007\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 999999\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: [SEP]\n",
      "  user_defined_symbols: [CLS]\n",
      "  user_defined_symbols: [MASK]\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 1\n",
      "  bos_id: 2\n",
      "  eos_id: 3\n",
      "  pad_id: 0\n",
      "  unk_piece: [UNK]\n",
      "  bos_piece: [BOS]\n",
      "  eos_piece: [EOS]\n",
      "  pad_piece: [PAD]\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/bert_pretrain/data/kowiki.txt\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(140) LOG(INFO) Loaded 2000000 lines\n",
      "trainer_interface.cc(117) LOG(WARNING) Too many sentences are loaded! (2451287), which may slow down training.\n",
      "trainer_interface.cc(119) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(122) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 2451287 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [PAD]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [UNK]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [BOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [EOS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [SEP]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [CLS]\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: [MASK]\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=287452241\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=4411\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 2450254 sentences.\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 2450254\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 7050692\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1781571 min_freq=424\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=576838 size=20 all=581927 active=38577 piece=▁아\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=390836 size=40 all=591445 active=48095 piece=▁유\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=297873 size=60 all=601378 active=58028 piece=에는\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=244712 size=80 all=609974 active=66624 piece=▁성\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=194372 size=100 all=616449 active=73099 piece=까지\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=193674 min_freq=462\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=176838 size=120 all=625299 active=38770 piece=▁우\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=154294 size=140 all=632274 active=45745 piece=▁파\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=140625 size=160 all=639734 active=53205 piece=00\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=125983 size=180 all=645481 active=58952 piece=▁요\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=114855 size=200 all=649839 active=63310 piece=리아\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=114086 min_freq=457\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=106760 size=220 all=657338 active=39316 piece=▁같은\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=100770 size=240 all=662564 active=44542 piece=▁왕\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=96037 size=260 all=670536 active=52514 piece=▁목\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=88392 size=280 all=675441 active=57419 piece=▁f\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=81678 size=300 all=681701 active=63679 piece=▁선수\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=80870 min_freq=446\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=77144 size=320 all=686930 active=39163 piece=▁때문에\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=73218 size=340 all=691000 active=43233 piece=▁조선\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=68829 size=360 all=695717 active=47950 piece=▁천\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=64009 size=380 all=700839 active=53072 piece=▁196\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=60953 size=400 all=706675 active=58908 piece=▁돌\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=60762 min_freq=435\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=58542 size=420 all=711350 active=39712 piece=▁다시\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=55779 size=440 all=715377 active=43739 piece=▁K\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=52528 size=460 all=721839 active=50201 piece=▁모두\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=49784 size=480 all=727356 active=55718 piece=▁히\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=47637 size=500 all=733038 active=61400 piece=▁전쟁\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=47558 min_freq=423\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=45919 size=520 all=738287 active=41703 piece=▁있어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=44025 size=540 all=743886 active=47302 piece=▁중심\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=42378 size=560 all=748357 active=51773 piece=▁N\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=40922 size=580 all=752114 active=55530 piece=▁H\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=39922 size=600 all=755142 active=58558 piece=le\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=39768 min_freq=414\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=38924 size=620 all=761204 active=43650 piece=▁검\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=37771 size=640 all=767376 active=49822 piece=란드\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=36292 size=660 all=772837 active=55283 piece=정을\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=35134 size=680 all=778715 active=61161 piece=▁설립\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=34016 size=700 all=783719 active=66165 piece=▁역사\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=34003 min_freq=400\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=33144 size=720 all=787243 active=42507 piece=▁만들어\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=32383 size=740 all=791538 active=46802 piece=▁시간\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=31645 size=760 all=795131 active=50395 piece=▁측\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30941 size=780 all=798845 active=54109 piece=과의\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=30041 size=800 all=803050 active=58314 piece=도는\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=30023 min_freq=392\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=29448 size=820 all=808546 active=45099 piece=▁난\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28836 size=840 all=813895 active=50448 piece=▁21\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=28270 size=860 all=818654 active=55207"
     ]
    }
   ],
   "source": [
    "\n",
    "#spm.SentencePieceTrainer.train(f\"--input={corpus_file} \\\n",
    "                               --model_prefix={model_prefix} \\\n",
    "                               --vocab_size={vocab_size + 7} \\\n",
    "                               --model_type=bpe \\\n",
    "                               --max_sentence_length=999999 \\\n",
    "                               --pad_id=0 --pad_piece=[PAD] \\\n",
    "                               --unk_id=1 --unk_piece=[UNK] \\\n",
    "                               --bos_id=2 --bos_piece=[BOS] \\\n",
    "                               --eos_id=3 --eos_piece=[EOS] \\\n",
    "                               --user_defined_symbols=[SEP],[CLS],[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57922991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상 토큰 수: 8000\n",
      "예시: ['▁이', '으로', '에서', '▁있', '▁2', '▁그', '▁대', '▁사', '이다']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_prefix}.model\")\n",
    "\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "# 확인\n",
    "print(f\"정상 토큰 수: {len(vocab_list)}\")\n",
    "print(\"예시:\", vocab_list[1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a49cbc",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리 (1) MASK 생성\n",
    "BERT의 MLM에 필요한 빈칸(mask)을 학습 데이터 전체 토큰의 15% 정도로 만들어 주세요. \n",
    "그 중 80%는 [MASK] 토큰, 10%는 랜덤한 토큰, 나머지 10%는 원래의 토큰을 그대로 사용하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "370503f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할 (띄어쓰기)\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # random mask를 위해서 순서를 섞음 (shuffle)\n",
    "    random.shuffle(cand_idx)\n",
    "    \n",
    "    # mask 실행\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "              break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "              continue\n",
    "        dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            \n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index], \"masked\": masked_token})\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출 (sorted 사용)\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    #return tokens, mask_lms, mask_idx, mask_label\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022db85",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리 (2) NSP pair 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae934fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):  \n",
    "            del tokens_a[0]                      # A가 더 길면 앞에서 하나 제거\n",
    "        else:\n",
    "            tokens_b.pop()                       # B가 더 길거나 같으면 뒤에서 하나 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6a33119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for i in range(len(doc)):  # doc 전체를 loop\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "                \n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "                \n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        \n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "        \n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "       \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9376982",
   "metadata": {},
   "source": [
    "## 4. 데이터 전처리 (3) 데이터셋 완성\n",
    "BERT pretrain 데이터셋을 생성해, json 포맷으로 저장하세요. \n",
    "데이터셋의 사이즈가 크므로np.memmap을 사용해 메모리 사용량을 최소화해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c463283f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95f91e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):        # 생성되는 단어 목록이 unknown인 경우는 제거합니다. \n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    # [[YOUR CODE]]\n",
    "                    if 0 < len(doc):\n",
    "                        # save\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                        \n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    # [[YOUR CODE]]\n",
    "                    pieces = vocab.encode_as_pieces(line) \n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "                    \n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                # [[YOUR CODE]]\n",
    "                # save\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d0799f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f3e88d01cc4426b1292fe71c971d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3957761 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "918189"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)\n",
    "\n",
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8be3b8",
   "metadata": {},
   "source": [
    "### json file 로딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40feae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef29c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5459979f76fe499fa43e1c60b2cbb5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_62/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_62/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72be2f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,    6,    6,  207, 3714,    4, 3324, 1042,    6,    6,    6,\n",
       "            6,  207, 3714,   37, 3418,  416,  810, 3666, 3625,  131, 3662,\n",
       "            7, 3629,    6,  241, 3602, 1114, 3724,  788,  243,   49, 3632,\n",
       "          796,  663, 1647, 3682, 3682, 3625,    6, 3008, 3625, 3616,   16,\n",
       "         3599,    6,    6,  207, 3714, 3602, 1755, 3630, 3646,  630, 3714,\n",
       "         3565, 3835,  429, 3740, 3628, 3626, 1369,   10, 5736, 2603, 1755,\n",
       "         3630,   41, 3644,  830, 3624, 1135,   52, 3599,   13,    6,   87,\n",
       "         1501, 2247,   25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636,\n",
       "         3779, 3601,  249, 3725, 1232,    6,    6,    6,  479, 3652, 3625,\n",
       "          243, 2780,   14, 1509,  168, 3877,  414,    6, 1697, 4290, 3873,\n",
       "         3703, 3683,    6,   21, 5007,  399, 1927, 3607,  813,   17, 3599,\n",
       "          307,  587,  931,  103, 4313, 4290,    4], dtype=int32),\n",
       " memmap([   5, 3676,  848, 3784, 1931,   58, 3676,  416, 2316, 3619, 3625,\n",
       "         3617, 3744, 4335,   12, 3625, 3616,  175, 3662,    7, 3629,  203,\n",
       "            6,    6,    6,    6,    6,    6,  143, 3625, 3616,  131, 3662,\n",
       "          342, 3629, 3616, 3602,  176,  334,  829, 1115, 3665,    6,    6,\n",
       "         3451, 1633,  375,  671, 1644, 3608,  547, 3423,  765,  815, 3604,\n",
       "            6,    6,    6, 2375, 3608, 3604,  532, 2589, 3599,    4,  307,\n",
       "          323,    6,  321, 3611,  622,  122, 3725, 3620, 3627, 3837, 3608,\n",
       "            6,  176,  268, 4082,   94,  567, 4014, 3617, 7474, 3616, 3830,\n",
       "           66, 3590,  307,  192, 1272,  158, 3788,  353, 3599,  202,  316,\n",
       "         3600,  176,   10,  323,  476, 3663, 1329,  605,  238, 3631, 2470,\n",
       "         3604, 1939,  106, 3627,   13,    6,    6, 1128,   48,    6,    6,\n",
       "          848, 3784, 3833,    8, 3637, 2263,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 1,\n",
       " memmap([   0,   18, 3686,    0,    0,    0,    0,    0,  103, 3610, 3686,\n",
       "         3718,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,  203,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,  203,    0,    0,    0,    0,\n",
       "            0,   18, 3686,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 1605, 3599,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,   81,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,   33,   52, 3599,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,  165,    0,    0,    0,\n",
       "            0,    0,  593,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          578, 3652, 3625, 3617, 4148, 3665,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0, 1381, 4148,\n",
       "         3451,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          752, 3608, 3604,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, 2143,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          347,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,  162,  490,    0,    0,   28, 3599,\n",
       "            0,    0,    0,    0,    0,    0,    0], dtype=int32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42b72a4",
   "metadata": {},
   "source": [
    "### train set과 validation set 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f986ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(2, 128000, 128)\n"
     ]
    }
   ],
   "source": [
    "print(type(pre_train_inputs))\n",
    "print(np.shape(pre_train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d48caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이타 셋 :  102400\n",
      "테스트 데이타 셋 :  25600\n"
     ]
    }
   ],
   "source": [
    "# pre_train_inputs : (enc_tokens, segments), \n",
    "# pre_train_labels : (labels_nsp, labels_mlm)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_tokens, segments = pre_train_inputs\n",
    "labels_nsp, labels_mlm = pre_train_labels\n",
    "\n",
    "train_tokens, test_tokens, train_segments, test_segments, train_nsp, test_nsp, train_mlm, test_mlm = \\\n",
    "    train_test_split( enc_tokens, segments, labels_nsp, labels_mlm, \n",
    "                        test_size=0.2, random_state=42 )\n",
    "\n",
    "train_inputs = (train_tokens, train_segments)\n",
    "test_inputs = (test_tokens, test_segments)\n",
    "train_labels = (train_nsp, train_mlm)\n",
    "test_labels = (test_nsp, test_mlm)\n",
    "\n",
    "print('훈련 데이타 셋 : ', len(train_tokens))\n",
    "print('테스트 데이타 셋 : ', len(test_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a006431",
   "metadata": {},
   "source": [
    "## 5. BERT 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81372909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d1ce750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6703787",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eff7cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc6bf0",
   "metadata": {},
   "source": [
    "### Shared Embedding\n",
    "\n",
    "1. 파라미터 수 감소 (Parameter Efficiency)\n",
    "     BERT는 **입력 임베딩 (Embedding Layer)**과 **출력 소프트맥스 레이어 (Output Layer)**에서 동일한 vocabulary embedding weight를 사용합니다.\n",
    "\n",
    "    * 입력: 토큰 → 벡터\n",
    "    * 출력: 벡터 → 토큰 (softmax)\n",
    "\n",
    "    둘 다 vocabulary size × hidden size (vocab_size × d_model)의 weight matrix를 사용하므로, \n",
    "    별도로 두 개를 만들면 매우 많은 파라미터가 필요합니다.\n",
    "    → 하나로 공유하면 파라미터 수가 절반으로 줄어듭니다.\n",
    "    \n",
    "\n",
    "2. 학습 안정성과 일반화 성능 향상\n",
    "    * 모델이 입력 표현과 출력 표현을 동일한 공간에서 해석하게 되어 일관성이 생기고,\n",
    "\n",
    "    * 특히 language modeling처럼 출력이 입력의 연장선인 문제에서는 더 효과적입니다.\n",
    "\n",
    "    즉, 단어의 의미를 임베딩과 디코딩에서 공유함으로써, 일반화 성능이 좋아지고 과적합 가능성이 줄어듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db608ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 레이어와 출력 softmax 레이어에서 공유되는 가중치 \n",
    "# - 입력 임베딩 시에는 input_ids → shared_weights lookup\n",
    "# - 출력 예측 시에는 decoder output → shared_weights^T → softmax\n",
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):                      # tf.keras.layers.Layer 클래스가 처음 call()될 때 자동 호출\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(     # 이 레이어에 학습 가능한 가중치를 추가하는 Keras 메서드\n",
    "                \"weights\",                             # self.add_weight로 생성된 가중치는 model.trainable_variables에 자동으로 포함\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        # inputs : [batch_size, seq_len]\n",
    "        # self.shared_weights : shape: [vocab_size, d_model]인 임베딩 가중치 행렬 \n",
    "        # tf.gather(..., indices) : indices에 해당하는 행(벡터)만 뽑아오는 함수\n",
    "        # 즉, 전체 vocab 임베딩에서, inputs에 토큰에 대해서만 뽑아옴(lookup)\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed \n",
    "        # shape: [batch_size, seq_len, d_model]\n",
    "        # 각 토큰을 d_model 차원의 벡터로 임베딩\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        \n",
    "        # self.shared_weights : [vocab_size, d_model]인 임베딩 가중치 행렬과 matmul하려고\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        \n",
    "        # decoder output * shared_weights^T \n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        \n",
    "        # softmax를 하려고 n_vocab로 변경\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98c81f",
   "metadata": {},
   "source": [
    "**학습 가능한 위치 임베딩 (learnable position embedding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c8714fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        # tf.ones_like(inputs) : inputs와 같은 shape의 텐서를 만들되, 모든 값을 1로 채운다. \n",
    "        # tf.math.cumsum(..., axis=1, exclusive=True) : 누적합을 구하되, 자기자신은 포함하지 않음 (즉, 앞의 값들만 합함)\n",
    "        # --> 각 토큰의 위치 인덱시 [0,1,2,3,4,...]\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "        # [batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78159bd5",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45d3c865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b3228e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_mode)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0731567",
   "metadata": {},
   "source": [
    "**포지션-와이즈 피드포워드 네트워크(Position-wise Feed-Forward Network, FFN)**\n",
    "\n",
    "    각 시퀀스의 토큰 벡터에 독립적으로 적용\n",
    "\n",
    "    FFN(x)=W2( activation(W1(x) )\n",
    "    - W_1 : 입력을 넓힘 d_model-> d_ff\n",
    "    - W_2 : 다시 원래 차원으로 줄일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7fda423",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd1a45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efede89",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "    * Embedding=TokenEmbedding+PositionEmbedding+SegmentEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad5a384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]   # 각 문장의 첫 번째 위치 토큰(보통 [CLS])에 해당하는 벡터만 선택\n",
    "                                    # NSP (Next Sentence Prediction) 작업을 위한 입력으로 사용\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")  # MLM을 위한 로짓 \n",
    "        return logits_cls, logits_lm\n",
    "        # logits_cls: [CLS] 토큰의 최종 벡터 (shape: [batch_size, hidden_dim]) (hidden_dim = d_model)\n",
    "        # logits_lm: MLM을 위한 로짓 (shape: [batch_size, seq_len, vocab_size])\n",
    "\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        # 서로 다른 분포를 가진 임베딩 벡터들을 더하면, 각 차원별 값의 스케일이 불균형해질 수 있고\n",
    "        # 학습이 불안정해질 가능성이 생기므로, \n",
    "        # LayerNormalization으로 스케일 차이를 조정 (출력의 평균을 0, 분산을 1에 가깝게 맞춤)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8c1365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [CLS] 토큰에서 나온 문장 전체의 표현을 사용해\n",
    "# NSP (Next Sentence Prediction) 또는 다른 이진/다중 분류 작업을 처리하기 위한 작은 분류기(classifier)\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81608856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ecea1f",
   "metadata": {},
   "source": [
    "## 6. pretrain 진행\n",
    "loss, accuracy 함수를 정의하고 Learning Rate 스케쥴링을 구현한 후, 10 Epoch까지 모델 학습을 시켜보세요.\n",
    "학습을 진행할 때는 배치 사이즈에 유의하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4474dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83179c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c76f2c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67d2bb2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ea54a8",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e4abf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config :  {'d_model': 256, 'n_head': 4, 'd_head': 64, 'dropout': 0.1, 'd_ff': 1024, 'layernorm_epsilon': 0.001, 'n_layer': 3, 'n_seq': 256, 'n_vocab': 8007, 'i_pad': 0}\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 4485632     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 4,551,936\n",
      "Trainable params: 4,551,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "train_steps: 3200\n",
      "Epoch 1/2\n",
      "1600/1600 [==============================] - 232s 143ms/step - loss: 18.9086 - nsp_loss: 0.6449 - mlm_loss: 18.2638 - nsp_accuracy: 0.5957 - mlm_lm_acc: 0.1156 - val_loss: 17.8351 - val_nsp_loss: 0.6330 - val_mlm_loss: 17.2021 - val_nsp_accuracy: 0.6005 - val_mlm_lm_acc: 0.1274\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.11563, saving model to ./models/bert_pre_train_1.hdf5\n",
      "Epoch 2/2\n",
      "1600/1600 [==============================] - 229s 143ms/step - loss: 17.5796 - nsp_loss: 0.6205 - mlm_loss: 16.9591 - nsp_accuracy: 0.6298 - mlm_lm_acc: 0.1295 - val_loss: 17.6061 - val_nsp_loss: 0.6229 - val_mlm_loss: 16.9832 - val_nsp_accuracy: 0.6189 - val_mlm_lm_acc: 0.1295\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.11563 to 0.12952, saving model to ./models/bert_pre_train_1.hdf5\n"
     ]
    }
   ],
   "source": [
    "config = Config({\"d_model\": 256, \n",
    "                 \"n_head\": 4, \n",
    "                 \"d_head\": 64, \n",
    "                 \"dropout\": 0.1, \n",
    "                 \"d_ff\": 1024, \n",
    "                 \"layernorm_epsilon\": 0.001, \n",
    "                 \"n_layer\": 3, \n",
    "                 \"n_seq\": 256, \n",
    "                 \"n_vocab\": 0, \n",
    "                 \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "print('Config : ', config)\n",
    "\n",
    "# 모델 생성\n",
    "model_1 = build_model_pre_train(config)\n",
    "model_1.summary()\n",
    "\n",
    "#\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "\n",
    "# compile\n",
    "# nsp loss : sparse_categorical_crossentropy\n",
    "# mlm loss : lm_loss (예: MLM을 위한 사용자 정의 함수)\n",
    "#pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), \\\n",
    "#                       optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})\n",
    "model_1.compile(\n",
    "                loss={\"nsp\": tf.keras.losses.sparse_categorical_crossentropy, \n",
    "                      \"mlm\": lm_loss},\n",
    "                optimizer=optimizer,\n",
    "                metrics={\"nsp\": \"accuracy\", \n",
    "                         \"mlm\": lm_acc}\n",
    "                )\n",
    "\n",
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train_1.hdf5\",\\\n",
    "                monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", \\\n",
    "                ave_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history_1 = model_1.fit(train_inputs, train_labels,\n",
    "                              epochs=epochs, \n",
    "                              batch_size=batch_size, \n",
    "                              callbacks=save_weights,\n",
    "                              validation_data=(test_inputs, test_labels)\n",
    "                       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14fff62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'nsp_loss', 'mlm_loss', 'nsp_accuracy', 'mlm_lm_acc', 'val_loss', 'val_nsp_loss', 'val_mlm_loss', 'val_nsp_accuracy', 'val_mlm_lm_acc'])\n"
     ]
    }
   ],
   "source": [
    "print(history_1.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92e5d025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "history = history_1\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.plot(history.history['val_nsp_loss'], 'g-', label='val_nsp_loss')\n",
    "plt.plot(history.history['val_mlm_loss'], 'k--', label='val_mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_accuracy'], 'b-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'r--', label='mlm_acc')\n",
    "plt.plot(history.history['val_nsp_accuracy'], 'g-', label='val_nsp_acc')\n",
    "plt.plot(history.history['val_mlm_lm_acc'], 'k--', label='val_mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9b2fc",
   "metadata": {},
   "source": [
    "# <회고>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ce3cd2",
   "metadata": {},
   "source": [
    "### 루브릭\n",
    "\n",
    "1. 한글 코퍼스를 가공하여 BERT pretrain용 데이터셋을 잘 생성하였다.\t\n",
    "- MLM, NSP task의 특징이 잘 반영된 pretrain용 데이터셋 생성과정이 체계적으로 진행되었다.\n",
    "2. 구현한 BERT 모델의 학습이 안정적으로 진행됨을 확인하였다.\t\n",
    "- 학습진행 과정 중에 MLM, NSP loss의 안정적인 감소가 확인되었다.\n",
    "3. 1M짜리 mini BERT 모델의 제작과 학습이 정상적으로 진행되었다.\t\n",
    "- 학습된 모델 및 학습과정의 시각화 내역이 제출되었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
